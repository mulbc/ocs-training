<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Deploying OpenShift Container Storage using Local Devices :: OCS Training</title>
    <link rel="canonical" href="https://mulbc.github.io/ocs-training/install&configure/ocp4ocs4/local-storage-byid.html">
    <meta name="generator" content="Antora 2.3.3">
    <link rel="stylesheet" href="../../_/css/site.css">
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://mulbc.github.io/ocs-training">OCS Training</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="#">Home</a>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Products</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="#">Product A</a>
            <a class="navbar-item" href="#">Product B</a>
            <a class="navbar-item" href="#">Product C</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Services</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="#">Service A</a>
            <a class="navbar-item" href="#">Service B</a>
            <a class="navbar-item" href="#">Service C</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Resources</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="#">Resource A</a>
            <a class="navbar-item" href="#">Resource B</a>
            <a class="navbar-item" href="#">Resource C</a>
          </div>
        </div>
        <div class="navbar-item">
          <span class="control">
            <a class="button is-primary" href="#">Download</a>
          </span>
        </div>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="install&amp;configure" data-version="master">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">OCS Installation and Configuration</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Module Overview</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="index.html">Overview</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ocs.html">OCS 4 Install and Set Up</a>
  </li>
  <li class="nav-item is-current-page" data-depth="1">
    <a class="nav-link" href="local-storage-byid.html">Local Storage Set Up</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../ocs4perf/ocs4perf.html">OCS Performance benchmarking</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">OCS Installation and Configuration</span>
    <span class="version">master</span>
  </div>
  <ul class="components">
    <li class="component">
      <span class="title">OCS Best Practices</span>
      <ul class="versions">
        <li class="version is-latest">
          <a href="../../bestPractice/index.html">master</a>
        </li>
      </ul>
    </li>
    <li class="component is-current">
      <span class="title">OCS Installation and Configuration</span>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">master</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">OCS Installation and Configuration</a></li>
    <li><a href="local-storage-byid.html">Local Storage Set Up</a></li>
  </ul>
</nav>
  <div class="edit-this-page"><a href="https://github.com/mulbc/ocs-training/edit/master/install&amp;configure/modules/ocp4ocs4/pages/local-storage-byid.adoc">Edit this Page</a></div>
  </div>
  <div class="content">
<article class="doc">
<h1 class="page">Deploying OpenShift Container Storage using Local Devices</h1>
<div id="toc" class="toc">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_installing_openshift_container_storage_operator_using_operator_hub">1. Installing OpenShift Container Storage Operator using Operator Hub</a></li>
<li><a href="#_installing_openshift_container_storage_on_local_storage_devices">2. Installing OpenShift Container Storage on Local Storage Devices</a>
<ul class="sectlevel2">
<li><a href="#_prerequisites">2.1. Prerequisites</a>
<ul class="sectlevel3">
<li><a href="#_label_verification">2.1.1. Label Verification</a></li>
</ul>
</li>
<li><a href="#_installing_the_local_storage_operator">2.2. Installing the Local Storage Operator</a></li>
<li><a href="#_finding_available_storage_devices">2.3. Finding Available Storage Devices</a></li>
<li><a href="#_creating_the_ocs_storage_cluster_on_aws">2.4. Creating the OCS Storage Cluster on AWS</a></li>
<li><a href="#_creating_the_ocs_storage_cluster_on_vmware">2.5. Creating the OCS Storage Cluster on VMware</a></li>
<li><a href="#_creating_the_ocs_storage_cluster_on_bare_metal">2.6. Creating the OCS Storage Cluster on Bare Metal</a></li>
<li><a href="#_validating_the_successful_ocs_cluster_creation">2.7. Validating the successful OCS Cluster creation</a></li>
</ul>
</li>
<li><a href="#_scaling_out_storage_by_adding_nodes_to_openshift_container_storage">3. Scaling out Storage by adding Nodes to OpenShift Container Storage</a></li>
<li><a href="#_scaling_up_storage_by_adding_devices_to_openshift_container_storage">4. Scaling up Storage by adding Devices to OpenShift Container Storage</a></li>
<li><a href="#_replacing_a_failed_device_with_openshift_container_storage">5. Replacing a Failed Device with OpenShift Container Storage</a>
<ul class="sectlevel2">
<li><a href="#_removing_failed_osd_from_ceph_cluster">5.1. Removing failed OSD from Ceph cluster</a></li>
<li><a href="#_delete_pvc_resources_associated_with_failed_osd">5.2. Delete PVC resources associated with failed OSD</a></li>
<li><a href="#_replace_failed_drive_and_create_new_pv">5.3. Replace failed drive and create new PV</a></li>
<li><a href="#_create_new_osd_for_new_device">5.4. Create new OSD for new device</a></li>
</ul>
</li>
<li><a href="#_using_the_rook_ceph_toolbox_to_validate_ceph_backing_storage">6. Using the Rook-Ceph toolbox to Validate Ceph backing storage</a></li>
</ul>
</div>
<div class="sect1">
<h2 id="_installing_openshift_container_storage_operator_using_operator_hub"><a class="anchor" href="#_installing_openshift_container_storage_operator_using_operator_hub"></a>1. Installing OpenShift Container Storage Operator using Operator Hub</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Start with creating the <code>openshift-storage</code> namespace.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc create namespace openshift-storage</code></pre>
</div>
</div>
<div class="paragraph">
<p>You must add the monitoring label to this namespace. This is required to get prometheus metrics and alerts for the OCP storage dashboards. To label the <code>openshift-storage</code> namespace use the following command:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc label namespace openshift-storage "openshift.io/cluster-monitoring=true"</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now switch over to your <strong>OpenShift Web Console</strong>. You can get your URL by issuing command below to get the OCP 4 <code>console</code> route.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get -n openshift-console route console</code></pre>
</div>
</div>
<div class="paragraph">
<p>Once you are logged in, navigate to the <strong>OperatorHub</strong> menu.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS-OCP-OperatorHub.png" alt="OCP OperatorHub">
</div>
<div class="title">Figure 1. OCP OperatorHub</div>
</div>
<div class="paragraph">
<p>Now type <code>openshift container storage</code> in the <strong>Filter by <em>keyword&#8230;&#8203;</em></strong> box.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-OCP-OperatorHub-Filter.png" alt="OCP OperatorHub Filter">
</div>
<div class="title">Figure 2. OCP OperatorHub filter on OpenShift Container Storage Operator</div>
</div>
<div class="paragraph">
<p>Select <code>OpenShift Container Storage Operator</code> and then select <strong>Install</strong>.</p>
</div>
<div class="paragraph">
<p>On the next screen make sure the settings are as shown in this figure. Also, make sure to change to <code>A specific namespace on the cluster</code> and chose namespace <code>openshift-storage</code>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-OCP-OperatorHub-Subscribe.png" alt="OCP OperatorHub Subscribe">
</div>
<div class="title">Figure 3. OCP Subscribe to OpenShift Container Storage</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Do not change any configuration other than the namespace as shown above.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Click <code>Subscribe</code>.</p>
</div>
<div class="paragraph">
<p>Now you can go back to your terminal window to check the progress of the installation.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">watch oc -n openshift-storage get csv</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME                            DISPLAY                       VERSION   REPLACES   PHASE
lib-bucket-provisioner.v1.0.0   lib-bucket-provisioner        1.0.0                Succeeded
ocs-operator.v4.4.0             OpenShift Container Storage   4.4.0                Succeeded</pre>
</div>
</div>
<div class="paragraph">
<p>You can exit by pressing <span class="keyseq"><kbd>Ctrl</kbd>+<kbd>C</kbd></span></p>
</div>
<div class="paragraph">
<p>The resource <code>csv</code> is a shortened word for <code>clusterserviceversions.operators.coreos.com</code>.</p>
</div>
<div class="admonitionblock caution">
<table>
<tr>
<td class="icon">
<i class="fa icon-caution" title="Caution"></i>
</td>
<td class="content">
<div class="title">Please wait until both operators <code>PHASE</code> changes to <code>Succeeded</code></div>
Reaching this state can take several minutes.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>You will now also see operator pods have been started in the <code>openshift-storage</code> namespace:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc -n openshift-storage get pods</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME                                      READY   STATUS    RESTARTS   AGE
lib-bucket-provisioner-55f74d96f6-mkrz2   1/1     Running   0          3m48s
noobaa-operator-68c8fbdff5-kgxxt          1/1     Running   0          3m41s
ocs-operator-78d98758db-dwc8h             1/1     Running   0          3m41s
rook-ceph-operator-55cb44fcfc-wxppc       1/1     Running   0          3m41s</pre>
</div>
</div>
<div class="paragraph">
<p>Navigate to the <code>Operators</code> menu on the left and select <code>Installed Operators</code>. Make sure the selected project at the top of the UI pane is set to <code>openshift-storage</code>. What you see should be similar to the following example:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCP4-installed-operators.png" alt="Openshift showing the installed operators in namespace openshift-storage">
</div>
</div>
<div class="paragraph">
<p>Now that the operator pods are running, you can create a Storage Cluster.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_installing_openshift_container_storage_on_local_storage_devices"><a class="anchor" href="#_installing_openshift_container_storage_on_local_storage_devices"></a>2. Installing OpenShift Container Storage on Local Storage Devices</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Use this procedure to install OpenShift Container Storage (OCS) on bare metal, Amazon EC2, and VMware infrastructures where OpenShift Container Platform 4 (OCP) is already installed.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The following sections use YAML files and CLI to create resources used to deploy OCS 4 on local storage devices. In future releases of OCP 4 and OCS 4 these operations will have the option of being done using the <strong>Openshift Web Console</strong> UI making using local storage easier and less error prone.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Installing OpenShift Container Storage Amazon EC2 using local storage operator is a Technology Preview feature. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="_prerequisites"><a class="anchor" href="#_prerequisites"></a>2.1. Prerequisites</h3>
<div class="ulist">
<ul>
<li>
<p>You must have at least 3 OCP worker nodes in the cluster with locally attached storage devices on each of them.</p>
</li>
<li>
<p>Each worker node must have a minimum of 12 CPUs and 64 GB Memory.</p>
</li>
<li>
<p>Each of the 3 worker nodes must have at least one available raw block device available to be used by OCS (e.g., 100 GB size).</p>
</li>
<li>
<p>You must have a minimum of three labeled worker nodes.</p>
<div class="ulist">
<ul>
<li>
<p>Each OCP worker node must have a specific label to deploy OCS <strong>Pods</strong>. To label the nodes use the following command:</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc label node &lt;NodeName&gt; cluster.ocs.openshift.io/openshift-storage=''</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Make sure to add this label to all OCP workers that have local storage devices to be used by OCS.
</td>
</tr>
</table>
</div>
<div class="ulist">
<ul>
<li>
<p>No other storage providers managing locally mounted storage on the storage nodes should be present that will conflict with the Local Storage Operator (LSO).</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="_label_verification"><a class="anchor" href="#_label_verification"></a>2.1.1. Label Verification</h4>
<div class="paragraph">
<p>Amazon EC2 zone and region topology labels are dynamically applied to OCP nodes by the AWS Cloud Provider. In VMware or bare metal environments, rack topology labels can be applied by a cluster administrator prior to OCS being deployed. OCS inspects zone and rack topology labels and uses them to inform placement policies for data availability and durability.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>OCS requires at least three failure domains for data safety and the domains should be <strong>symmetrical</strong> in terms of node quantity. If the OCP nodes used for the OCS deployment do not have preexisting topology labels OCS will generate three virtual racks using <code>topology.rook.io/rack</code> topology labels.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The following command will output a list of nodes with the OCS label, and print a column for each of the topology labels OCS takes into consideration.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get nodes -L failure-domain.beta.kubernetes.io/zone,failure-domain.beta.kubernetes.io/rack,failure-domain.kubernetes.io/zone,failure-domain.kubernetes.io/rack -l cluster.ocs.openshift.io/openshift-storage=''</code></pre>
</div>
</div>
<div class="paragraph">
<p>If the output from this command does not print <strong>any</strong> topology labels, then it is safe to proceed.</p>
</div>
<div class="paragraph">
<p>If the output from this command shows at least three existing unique topology labels (eg. three different racks, or three different zones), then it is safe to proceed.</p>
</div>
<div class="paragraph">
<p>If there are existing rack labels <strong>and</strong> there are less than 3 different values (e.g., 2 nodes in rack1 and 1 node in rack2 only), then different nodes should be labeled for OCS.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_installing_the_local_storage_operator"><a class="anchor" href="#_installing_the_local_storage_operator"></a>2.2. Installing the Local Storage Operator</h3>
<div class="paragraph">
<p>Start with creating the <code>local-storage</code> namespace.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc new-project local-storage</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now switch over to your <strong>Openshift Web Console</strong> and select <strong>OperatorHub</strong>. Type <code>local storage</code> in the <strong>Filter by <em>keyword&#8230;&#8203;</em></strong> box.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCP4-LSO-filter.png" alt="OperatorHub LSO Operator filter">
</div>
</div>
<div class="paragraph">
<p>Select <code>Local Storage Operator</code> and then select <strong>Install</strong>.</p>
</div>
<div class="paragraph">
<p>On the next screen make sure the settings are as shown in this figure. Also, make sure to change to <code>A specific namespace on the cluster</code> and chose namespace <code>local-storage</code>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-OCP-OperatorHub-LSO-Subscribe.png" alt="OCP OperatorHub Subscribe">
</div>
<div class="title">Figure 4. OCP Subscribe to Local Storage Operator</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Do not change any configuration other than the namespace as shown above.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Click <code>Subscribe</code>.</p>
</div>
<div class="paragraph">
<p>Now you can go back to your terminal window to check the progress of the installation.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc -n local-storage get pods</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME                                     READY   STATUS    RESTARTS   AGE
local-storage-operator-765dc5b87-vfh69   1/1     Running   0          23s</pre>
</div>
</div>
<div class="paragraph">
<p>The Local Storage Operator (LSO) has been successfully installed. Now move on to creating local persistent volumes (PVs) on the storage nodes using LocalVolume Custom Resource (CR) files.</p>
</div>
</div>
<div class="sect2">
<h3 id="_finding_available_storage_devices"><a class="anchor" href="#_finding_available_storage_devices"></a>2.3. Finding Available Storage Devices</h3>
<div class="paragraph">
<p>Using LSO to create <strong>PVs</strong> can be done for bare metal, Amazon EC2, or VMware storage devices. What you must know is the exact device name on each of the 3 or more OCP worker nodes you labeled with OCS label <code>cluster.ocs.openshift.io/openshift-storage=''</code>. The method to do this is to logon to each node and verify the device names as well, the size of each device, and that the device is available.</p>
</div>
<div class="paragraph">
<p>Logon to each worker node that will be used for OCS resources and find the unique <code>by-id</code> device name for each available raw block device. You will want to copy these values to a clipboard for the next step.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc debug node/&lt;NodeName&gt;</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>oc debug node/ip-10-0-135-71.us-east-2.compute.internal
Starting pod/ip-10-0-135-71us-east-2computeinternal-debug ...
To use host binaries, run `chroot /host`
Pod IP: 10.0.135.71
If you don't see a command prompt, try pressing enter.
sh-4.2# chroot /host
sh-4.4# lsblk
NAME                         MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
xvda                         202:0    0   120G  0 disk
|-xvda1                      202:1    0   384M  0 part /boot
|-xvda2                      202:2    0   127M  0 part /boot/efi
|-xvda3                      202:3    0     1M  0 part
`-xvda4                      202:4    0 119.5G  0 part
  `-coreos-luks-root-nocrypt 253:0    0 119.5G  0 dm   /sysroot
nvme0n1                      259:0    0   1.7T  0 disk
nvme1n1                      259:1    0   1.7T  0 disk</pre>
</div>
</div>
<div class="paragraph">
<p>After you know which local devices are available, in this case <code>nvme0n1</code> and <code>nvme1n1</code>, you can now find the <code>by-id</code>, a unique name depending on the hardware serial number for each device.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>sh-4.4# ls -l /dev/disk/by-id/
total 0
lrwxrwxrwx. 1 root root 10 Mar 17 16:24 dm-name-coreos-luks-root-nocrypt -&gt; ../../dm-0
lrwxrwxrwx. 1 root root 13 Mar 17 16:24 nvme-Amazon_EC2_NVMe_Instance_Storage_AWS10382E5D7441494EC -&gt; ../../nvme0n1
lrwxrwxrwx. 1 root root 13 Mar 17 16:24 nvme-Amazon_EC2_NVMe_Instance_Storage_AWS60382E5D7441494EC -&gt; ../../nvme1n1
lrwxrwxrwx. 1 root root 13 Mar 17 16:24 nvme-nvme.1d0f-4157533130333832453544373434313439344543-416d617a6f6e20454332204e564d6520496e7374616e63652053746f72616765-00000001 -&gt; ../../nvme0n1
lrwxrwxrwx. 1 root root 13 Mar 17 16:24 nvme-nvme.1d0f-4157533630333832453544373434313439344543-416d617a6f6e20454332204e564d6520496e7374616e63652053746f72616765-00000001 -&gt; ../../nvme1n1</pre>
</div>
</div>
<div class="paragraph">
<p>In this case the EC2 instance type is i3.4xlarge so we know all 3 worker nodes are the same type of machine but their <code>by-id</code> identifier is unique for every local device. As shown above, the results of <code>lsblk</code> shows the last 2 devices <code>nvme0n1</code> and <code>nvme1n1</code> are available with a size of 1.7 TB.</p>
</div>
<div class="paragraph">
<p>For each worker node that has the OCS label (minimum 3) you will need to find the unique <code>by-id</code>. For this node they are:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>nvme-Amazon_EC2_NVMe_Instance_Storage_AWS10382E5D7441494EC</code></p>
</li>
<li>
<p><code>nvme-Amazon_EC2_NVMe_Instance_Storage_AWS60382E5D7441494EC</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This example just shows results for one node so this method needs to be repeated for the other nodes that have storage devices to be used by OCS.</p>
</div>
<div class="paragraph">
<p>The next sections will detail how to create and validate the OCS <strong>StorageCluster</strong> using Amazon EC2, VMware, and bare metal local storage devices.</p>
</div>
</div>
<div class="sect2">
<h3 id="_creating_the_ocs_storage_cluster_on_aws"><a class="anchor" href="#_creating_the_ocs_storage_cluster_on_aws"></a>2.4. Creating the OCS Storage Cluster on AWS</h3>
<div class="paragraph">
<p>The next step is to create the LSO LocalVolume CR which in turn will create <strong>PVs</strong> and a new <strong>StorageClass</strong> for creating Ceph storage. For this example only device <code>nvme0n1</code> will be used on each node using the <code>by-id</code> unique identifier in the CR.</p>
</div>
<div class="paragraph">
<p>Before you create this resource make sure you have labeled your OCP worker nodes with the OCS label.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get nodes -l cluster.ocs.openshift.io/openshift-storage -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}'</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>ip-10-0-135-71.us-east-2.compute.internal
ip-10-0-145-125.us-east-2.compute.internal
ip-10-0-160-91.us-east-2.compute.internal</pre>
</div>
</div>
<div class="paragraph">
<p>Now that you know a minimum of 3 nodes are labeled you can proceed. The label is important because it is used as the <code>nodeSelector</code> below.</p>
</div>
<div class="listingblock">
<div class="title">LocalVolume CR local-storage-block.yaml using OCS label as Node Selector and <code>by-id</code> device identifier:</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: local.storage.openshift.io/v1
kind: LocalVolume
metadata:
  name: local-block
  namespace: local-storage
spec:
  nodeSelector:
    nodeSelectorTerms:
    - matchExpressions:
        - key: cluster.ocs.openshift.io/openshift-storage
          operator: In
          values:
          - ""
  storageClassDevices:
    - storageClassName: localblock
      volumeMode: Block
      devicePaths:
        - /dev/disk/by-id/nvme-Amazon_EC2_NVMe_Instance_Storage_AWS10382E5D7441494EC   # &lt;-- modify this line
        - /dev/disk/by-id/nvme-Amazon_EC2_NVMe_Instance_Storage_AWS1F45C01D7E84FE3E9   # &lt;-- modify this line
        - /dev/disk/by-id/nvme-Amazon_EC2_NVMe_Instance_Storage_AWS136BC945B4ECB9AE4   # &lt;-- modify this line</code></pre>
</div>
</div>
<div class="paragraph">
<p>Create this LocalVolume CR using the following command:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc create -f local-storage-block.yaml</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>localvolume.local.storage.openshift.io/local-block created</pre>
</div>
</div>
<div class="paragraph">
<p>Now that the CR is created let&#8217;s see the results.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc -n local-storage get pods</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME                                     READY   STATUS    RESTARTS   AGE
local-block-local-diskmaker-kkp7j        1/1     Running   0          5m1s
local-block-local-diskmaker-nqcgl        1/1     Running   0          5m1s
local-block-local-diskmaker-szd72        1/1     Running   0          5m1s
local-block-local-provisioner-bsztg      1/1     Running   0          5m1s
local-block-local-provisioner-g9zgf      1/1     Running   0          5m1s
local-block-local-provisioner-gzktp      1/1     Running   0          5m1s
local-storage-operator-765dc5b87-vfh69   1/1     Running   0          53m</pre>
</div>
</div>
<div class="paragraph">
<p>There should now be a new <strong>PV</strong> for each of the local storage devices on the 3 worker nodes. Remember when we checked above there were 2 available storage devices per worker node. Only device <code>nvme0n1</code> was used on each worker node and the size is 1.7 TB.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get pv</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
local-pv-40bd1474   1769Gi     RWO            Delete           Available           localblock              5m53s
local-pv-66631f85   1769Gi     RWO            Delete           Available           localblock              5m52s
local-pv-c56e9c     1769Gi     RWO            Delete           Available           localblock              5m53s</pre>
</div>
</div>
<div class="paragraph">
<p>And finally we should have an additional <strong>StorageClass</strong> as a result of creating this LocalVolume CR. This <strong>StorageClass</strong> will be used when creating <strong>PVCs</strong> in the next step of creating a <strong>StorageCluster</strong>.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get sc</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME            PROVISIONER                    AGE
gp2 (default)   kubernetes.io/aws-ebs          7h14m
localblock      kubernetes.io/no-provisioner   7m46s</pre>
</div>
</div>
<div class="paragraph">
<p>For Amazon EC2 instance that have local storage devices (e.g., i3.4xlarge) we need to create a <strong>StorageCluster</strong> Custom Resource (CR) that will use the <code>localblock</code> <strong>StorageClass</strong> and 3 <strong>PVs</strong> created in the previous section.</p>
</div>
<div class="listingblock">
<div class="title">StorageCluster CR cluster-service-AWS.yaml using <code>gp2</code> and <code>localblock</code> storageclasses:</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: ocs.openshift.io/v1
kind: StorageCluster
metadata:
  name: ocs-storagecluster
  namespace: openshift-storage
spec:
  manageNodes: false
  monPVCTemplate:
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 10Gi
      storageClassName: gp2
      volumeMode: Filesystem
  storageDeviceSets:
  - count: 1
    dataPVCTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 1
        storageClassName: localblock
        volumeMode: Block
    name: ocs-deviceset
    placement: {}
    portable: false
    replica: 3
    resources: {}</code></pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The <code>storage</code> size for <strong>storageDeviceSets</strong> must be less than or equal to the size of the raw block devices. Setting the value to <code>1</code> will guarantee that this requirement is met.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Create this StorageCluster CR using the following command:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc create -f https://raw.githubusercontent.com/red-hat-storage/ocs-training/master/ocp4ocs4/yamls/cluster-service-AWS.yaml</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>storagecluster.ocs.openshift.io/ocs-storagecluster created</pre>
</div>
</div>
<div class="paragraph">
<p>Reference <a href="#_validating_the_successful_ocs_cluster_creation">Validating the successful OCS Cluster creation</a> for how to validate your <strong>StorageCluster</strong> deployment.</p>
</div>
</div>
<div class="sect2">
<h3 id="_creating_the_ocs_storage_cluster_on_vmware"><a class="anchor" href="#_creating_the_ocs_storage_cluster_on_vmware"></a>2.5. Creating the OCS Storage Cluster on VMware</h3>
<div class="paragraph">
<p>The process for using local storage devices for OCP on VMware environments is very similar but has some differences. The first is the types of local storage that is supported for VMware. The 3 types are the following:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>VMDK</p>
</li>
<li>
<p>Raw Device Mapping (RDM)</p>
</li>
<li>
<p>VMDirectPath</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In order to use local storage devices on VMware you must have a minimum of 3 worker nodes with the <code>same</code> storage type attached to each node.</p>
</div>
<div class="paragraph">
<p>Once this has been done you can use a similar method as done for AWS by logging on to the OCP worker nodes with the OCS label (see <a href="#_prerequisites">Prerequisites</a>) and issuing a <code>lsblk</code>. By inspecting the results of this command you will know which devices are available and what the size of each device is (e.g., /dev/sdb is 100 GB). See <a href="#_finding_available_storage_devices">Finding Available Storage Devices</a> for more details.</p>
</div>
<div class="paragraph">
<p>For Monitor storage in the StorageCluster CR use <strong>monDataDirHostPath</strong> and set its value to /var/lib/rook.</p>
</div>
<div class="paragraph">
<p>Now create the LocalVolume CR for Block <strong>PVs</strong>.</p>
</div>
<div class="listingblock">
<div class="title">LocalVolume CR local-storage-block.yaml using OCS label as Node Selector:</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: local.storage.openshift.io/v1
kind: LocalVolume
metadata:
  name: local-block
  namespace: local-storage
spec:
  nodeSelector:
    nodeSelectorTerms:
    - matchExpressions:
        - key: cluster.ocs.openshift.io/openshift-storage
          operator: In
          values:
          - ""
  storageClassDevices:
    - storageClassName: localblock
      volumeMode: Block
      devicePaths:
        - /dev/disk/by-id/scsi-36000c2991c27c2e5ba7c47d1e4352de2   # &lt;-- modify this line
        - /dev/disk/by-id/scsi-36000c29682ca9e347926406711f3dc4e   # &lt;-- modify this line
        - /dev/disk/by-id/scsi-36000c296aaf03a9b1e4b01d086bc6348   # &lt;-- modify this line</code></pre>
</div>
</div>
<div class="paragraph">
<p>Create this LocalVolume CR for Block <strong>PVs</strong> using the following command:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc create -f local-storage-block.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>Check for the new <code>localblock</code> <strong>StorageClass</strong>.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get sc | grep localblock</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME            PROVISIONER                     AGE
localblock      kubernetes.io/no-provisioner    8m38s</pre>
</div>
</div>
<div class="paragraph">
<p>After the new <strong>StorageClasses</strong> are created there will be new <strong>PVs</strong> with <code>Available</code> status.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get get pv</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
local-pv-150fdc87   100Gi      RWO            Delete           Available           localblock              2m11s
local-pv-183bfc0a   100Gi      RWO            Delete           Available           localblock              2m11s
local-pv-b2f5cb25   100Gi      RWO            Delete           Available           localblock              2m21s</pre>
</div>
</div>
<div class="paragraph">
<p>The last step for using local storage on VMware is to create the <strong>StorageCluster</strong>. This is again very similar to how it was done for AWS but with a few changes.</p>
</div>
<div class="listingblock">
<div class="title">StorageCluster CR cluster-service-VMware.yaml using <code>localblock</code> storageclass:</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: ocs.openshift.io/v1
kind: StorageCluster
metadata:
  name: ocs-storagecluster
  namespace: openshift-storage
spec:
  manageNodes: false
  monDataDirHostPath: /var/lib/rook
  storageDeviceSets:
  - count: 1
    dataPVCTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 1
        storageClassName: localblock
        volumeMode: Block
    name: ocs-deviceset
    placement: {}
    portable: false
    replica: 3
    resources: {}</code></pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The <code>storage</code> size for <strong>storageDeviceSets</strong> must be less than or equal to the size of the raw block devices. Setting the value to <code>1</code> for will guarantee that this requirement is met.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Create this StorageCluster CR using the following command:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc create -f https://raw.githubusercontent.com/red-hat-storage/ocs-training/master/ocp4ocs4/yamls/cluster-service-VMware.yaml</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>storagecluster.ocs.openshift.io/ocs-storagecluster created</pre>
</div>
</div>
<div class="paragraph">
<p>Reference <a href="#_validating_the_successful_ocs_cluster_creation">Validating the successful OCS Cluster creation</a> for how to validate your <strong>StorageCluster</strong> deployment.</p>
</div>
</div>
<div class="sect2">
<h3 id="_creating_the_ocs_storage_cluster_on_bare_metal"><a class="anchor" href="#_creating_the_ocs_storage_cluster_on_bare_metal"></a>2.6. Creating the OCS Storage Cluster on Bare Metal</h3>
<div class="paragraph">
<p>The process for using local storage devices for OCP on bare methal environments is very similar to both AWS and VMware environments with some differences.</p>
</div>
<div class="paragraph">
<p>In order to use local storage devices on bare metal servers you must have a minimum of 3 worker nodes. Each of the machines must have at least one raw block device and the <code>same</code> storage type attached to each node (.e.g., 2TB NVMe drive).</p>
</div>
<div class="paragraph">
<p>To identify the storage devices on each node use the same method as the one used for AWS and VMWare environments by logging on to each OCS worker node and issuing a lsblk command. By inspecting the results of this command you will know which devices are available and their size. See <a href="#_finding_available_storage_devices">Finding Available Storage Devices</a> for more details.</p>
</div>
<div class="paragraph">
<p>For Monitor storage in the StorageCluster CR use <strong>monDataDirHostPath</strong> and set its value to /var/lib/rook.</p>
</div>
<div class="paragraph">
<p>First step is to create the LocalVolume CR for Block <strong>PVs</strong>.</p>
</div>
<div class="listingblock">
<div class="title">LocalVolume CR local-storage-block.yaml using OCS label as Node Selector:</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: local.storage.openshift.io/v1
kind: LocalVolume
metadata:
  name: local-block
  namespace: local-storage
spec:
  nodeSelector:
    nodeSelectorTerms:
    - matchExpressions:
        - key: cluster.ocs.openshift.io/openshift-storage
          operator: In
          values:
          - ""
  storageClassDevices:
    - storageClassName: localblock
      volumeMode: Block
      devicePaths:
        - /dev/disk/by-id/nvme-INTEL_SSDPEKKA128G7_BTPY81260978128A   # &lt;-- modify this line
        - /dev/disk/by-id/nvme-INTEL_SSDPEKKA128G7_BTPY80440W5U128A   # &lt;-- modify this line
        - /dev/disk/by-id/nvme-INTEL_SSDPEKKA128G7_BTPYB85AABDE128A   # &lt;-- modify this line
        - /dev/disk/by-id/nvme-INTEL_SSDPEKKA128G7_BTPY0A60CB81128A   # &lt;-- modify this line
        - /dev/disk/by-id/nvme-INTEL_SSDPEKKA128G7_BTPY0093D45E128A   # &lt;-- modify this line
        - /dev/disk/by-id/nvme-INTEL_SSDPEKKA128G7_BTPYE46F6060128A   # &lt;-- modify this line</code></pre>
</div>
</div>
<div class="paragraph">
<p>Create this LocalVolume CR for Block <strong>PVs</strong> using the following command:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc create -f local-storage-file.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>Check for the new <code>localblock</code> <strong>StorageClass</strong>.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get sc | grep localblock</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME            PROVISIONER                     AGE
localblock      kubernetes.io/no-provisioner    10m20s</pre>
</div>
</div>
<div class="paragraph">
<p>The last step for using local storage on bare metal servers is to create the <strong>StorageCluster</strong>. This is again very similar to how it was done for AWS and VMware.</p>
</div>
<div class="listingblock">
<div class="title">StorageCluster CR cluster-service-metal.yaml using <code>monDataDirHostPath</code> and <code>localblock</code> storageclass:</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: ocs.openshift.io/v1
kind: StorageCluster
metadata:
  name: ocs-storagecluster
  namespace: openshift-storage
spec:
  manageNodes: false
  monDataDirHostPath: /var/lib/rook
  storageDeviceSets:
  - count: 1
    dataPVCTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 1
        storageClassName: localblock
        volumeMode: Block
    name: ocs-deviceset
    placement: {}
    portable: false
    replica: 3
    resources: {}</code></pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The <code>storage</code> size for <strong>storageDeviceSets</strong> must be less than or equal to the size of the raw block devices. Setting the value to <code>1</code> for both will guarantee that this requirement is met.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Create this StorageCluster CR using the following command:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc create -f https://raw.githubusercontent.com/red-hat-storage/ocs-training/master/ocp4ocs4/yamls/cluster-service-metal.yaml</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>storagecluster.ocs.openshift.io/ocs-storagecluster created</pre>
</div>
</div>
<div class="paragraph">
<p>Reference <a href="#_validating_the_successful_ocs_cluster_creation">Validating the successful OCS Cluster creation</a> for how to validate your <strong>StorageCluster</strong> deployment.</p>
</div>
</div>
<div class="sect2">
<h3 id="_validating_the_successful_ocs_cluster_creation"><a class="anchor" href="#_validating_the_successful_ocs_cluster_creation"></a>2.7. Validating the successful OCS Cluster creation</h3>
<div class="paragraph">
<p>Once the <strong>StorageCluster</strong> is created OCS pods will start showing up in the <code>openshift-storage</code> namespace. For the deployment to completely finish could take up to 10 minutes so be patient. Below you will find examples of a successful deployment of the OCS <strong>Pods</strong> and <strong>PVCs</strong>.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc -n openshift-storage get pods</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME                                                                  READY   STATUS      RESTARTS   AGE
pod/csi-cephfsplugin-kzfrx                                            3/3     Running     0          7m49s
pod/csi-cephfsplugin-provisioner-67777bbbc9-j28s9                     5/5     Running     0          7m49s
pod/csi-cephfsplugin-provisioner-67777bbbc9-nrghg                     5/5     Running     0          7m49s
pod/csi-cephfsplugin-vm4qw                                            3/3     Running     0          7m49s
pod/csi-cephfsplugin-xzqc6                                            3/3     Running     0          7m49s
pod/csi-rbdplugin-9jvmd                                               3/3     Running     0          7m50s
pod/csi-rbdplugin-bzpb2                                               3/3     Running     0          7m50s
pod/csi-rbdplugin-provisioner-8569698c9b-hdzgh                        5/5     Running     0          7m49s
pod/csi-rbdplugin-provisioner-8569698c9b-ll9wm                        5/5     Running     0          7m49s
pod/csi-rbdplugin-tf68q                                               3/3     Running     0          7m50s
pod/lib-bucket-provisioner-55f74d96f6-mkrz2                           1/1     Running     0          162m
pod/noobaa-core-0                                                     1/1     Running     0          3m37s
pod/noobaa-db-0                                                       1/1     Running     0          3m37s
pod/noobaa-endpoint-679dfc8669-2cxt5                                  1/1     Running     0          2m12s
pod/noobaa-operator-68c8fbdff5-kgxxt                                  1/1     Running     0          162m
pod/ocs-operator-78d98758db-dwc8h                                     1/1     Running     0          162m
pod/rook-ceph-crashcollector-ip-10-0-135-71-7f4647b5f5-cp4nt          1/1     Running     0          4m35s
pod/rook-ceph-crashcollector-ip-10-0-145-125-f765fc64b-tnlrp          1/1     Running     0          5m42s
pod/rook-ceph-crashcollector-ip-10-0-160-91-5fb874cd6c-4bqvl          1/1     Running     0          6m29s
pod/rook-ceph-drain-canary-86f0e65050c75c523a149de3c6c7b27c-85f4255   1/1     Running     0          3m41s
pod/rook-ceph-drain-canary-a643022da9a50239ad6fc41164ccb7c4-7cnjt4n   1/1     Running     0          3m42s
pod/rook-ceph-drain-canary-e290c9c7dc116eb65fcb3ad57067aa65-54mgcfs   1/1     Running     0          3m38s
pod/rook-ceph-mds-ocs-storagecluster-cephfilesystem-a-7d7d5b5fxqdbs   1/1     Running     0          3m24s
pod/rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-6899b5b6znmtx   1/1     Running     0          3m23s
pod/rook-ceph-mgr-a-544b89b5c6-l6s2l                                  1/1     Running     0          4m14s
pod/rook-ceph-mon-a-b74c86ddf-dq25t                                   1/1     Running     0          5m15s
pod/rook-ceph-mon-b-7cb5446957-kxz4w                                  1/1     Running     0          4m51s
pod/rook-ceph-mon-c-56d689c77c-gb5n9                                  1/1     Running     0          4m35s
pod/rook-ceph-operator-55cb44fcfc-wxppc                               1/1     Running     0          162m
pod/rook-ceph-osd-0-74b8654667-kccs8                                  1/1     Running     0          3m42s
pod/rook-ceph-osd-1-7cc9444867-wzvmh                                  1/1     Running     0          3m41s
pod/rook-ceph-osd-2-5b5c4dcd57-tr5ck                                  1/1     Running     0          3m38s
pod/rook-ceph-osd-prepare-ocs-deviceset-0-0-dq89h-pzh4d               0/1     Completed   0          3m55s
pod/rook-ceph-osd-prepare-ocs-deviceset-1-0-wnbrp-7ls8b               0/1     Completed   0          3m55s
pod/rook-ceph-osd-prepare-ocs-deviceset-2-0-xst6j-mjpv7               0/1     Completed   0          3m55s</pre>
</div>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc -n openshift-storage get pvc</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME                                            STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                  AGE
persistentvolumeclaim/db-noobaa-db-0            Bound    pvc-99634049-ee21-490d-9fa7-927bbf3c87bc   50Gi       RWO            ocs-storagecluster-ceph-rbd   4m16s
persistentvolumeclaim/ocs-deviceset-0-0-dq89h   Bound    local-pv-40bd1474                          1769Gi     RWO            localblock                    4m35s
persistentvolumeclaim/ocs-deviceset-1-0-wnbrp   Bound    local-pv-66631f85                          1769Gi     RWO            localblock                    4m35s
persistentvolumeclaim/ocs-deviceset-2-0-xst6j   Bound    local-pv-c56e9c                            1769Gi     RWO            localblock                    4m35s
persistentvolumeclaim/rook-ceph-mon-a           Bound    pvc-0cc612ce-22ff-4f3c-bc0d-147e88d45df3   10Gi       RWO            gp2                           7m55s
persistentvolumeclaim/rook-ceph-mon-b           Bound    pvc-7c0187c1-1000-4d3b-8b31-d17235328082   10Gi       RWO            gp2                           7m44s
persistentvolumeclaim/rook-ceph-mon-c           Bound    pvc-e30645cd-1733-46c5-b0bf-566bdd0d2ab8   10Gi       RWO            gp2                           7m34s</pre>
</div>
</div>
<div class="paragraph">
<p>If we now look again at the <strong>PVs</strong> again you will see they are now in a <code>Bound</code> state verses <code>Available</code> as they were before OCS <strong>StorageCluster</strong> was created.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get pv | grep localblock</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>local-pv-40bd1474                          1769Gi     RWO            Delete           Bound       openshift-storage/ocs-deviceset-0-0-dq89h   localblock                             46m
local-pv-66631f85                          1769Gi     RWO            Delete           Bound       openshift-storage/ocs-deviceset-1-0-wnbrp   localblock                             46m
local-pv-c56e9c                            1769Gi     RWO            Delete           Bound       openshift-storage/ocs-deviceset-2-0-xst6j   localblock                             46m</pre>
</div>
</div>
<div class="paragraph">
<p>You can check the status of the storage cluster with the following:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get storagecluster -n openshift-storage</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME                 AGE   PHASE   CREATED AT             VERSION
ocs-storagecluster   14m   Ready   2020-03-11T22:52:04Z   4.3.0</pre>
</div>
</div>
<div class="paragraph">
<p>If it says <code>Ready</code> you can continue on to using OCS storage for applications.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_scaling_out_storage_by_adding_nodes_to_openshift_container_storage"><a class="anchor" href="#_scaling_out_storage_by_adding_nodes_to_openshift_container_storage"></a>3. Scaling out Storage by adding Nodes to OpenShift Container Storage</h2>
<div class="sectionbody">
<div class="paragraph">
<p>You must have three OCP worker nodes with the same storage type and size attached to each node (for example, 2TB NVMe drive) as the original OCS <strong>StorageCluster</strong> was created with.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Each OCP worker node must have a specific label to deploy OCS <strong>Pods</strong>. To label the nodes use the following command:</p>
</li>
</ul>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc label node &lt;NodeName&gt; cluster.ocs.openshift.io/openshift-storage=''</code></pre>
</div>
</div>
<div class="paragraph">
<p>Once the new nodes are labeled you are ready to add the new local storage device(s) available in these new worker nodes to the OCS <strong>StorageCluster</strong>. Follow the process in the <a href="#_scaling_up_storage_by_adding_devices_to_openshift_container_storage">next section</a> to create new <strong>PVs</strong> and increase the number of Ceph OSDs. The new OSDs (3 minimum) most likely will be scheduled by OpenShift on the new worker nodes with the OCS label.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_scaling_up_storage_by_adding_devices_to_openshift_container_storage"><a class="anchor" href="#_scaling_up_storage_by_adding_devices_to_openshift_container_storage"></a>4. Scaling up Storage by adding Devices to OpenShift Container Storage</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Use this procedure to add storage capacity (additional storage devices) to your configured Red Hat OpenShift Container Storage worker nodes.</p>
</div>
<div class="paragraph">
<p>To add storage capacity to existing OCP nodes with OCS installed, you will need to find the unique <code>by-id</code> identifier for available devices that you want to add, a minimum of one device per worker node. See <a href="#_finding_available_storage_devices">Finding Available Storage Devices</a> for more details. Make sure to do this process for all existing nodes (minimum of 3) that you want to add storage to.</p>
</div>
<div class="listingblock">
<div class="title">LocalVolume CR local-storage-block-expand.yaml using OCS label as Node Selector and <code>by-id</code> device identifier:</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: local.storage.openshift.io/v1
kind: LocalVolume
metadata:
  name: local-block
  namespace: local-storage
spec:
  nodeSelector:
    nodeSelectorTerms:
    - matchExpressions:
        - key: cluster.ocs.openshift.io/openshift-storage
          operator: In
          values:
          - ""
  storageClassDevices:
    - storageClassName: localblock
      volumeMode: Block
      devicePaths:
        - /dev/disk/by-id/nvme-Amazon_EC2_NVMe_Instance_Storage_AWS10382E5D7441494EC   # &lt;-- modify this line
        - /dev/disk/by-id/nvme-Amazon_EC2_NVMe_Instance_Storage_AWS60382E5D7441494EC   # &lt;-- modify this line
        - /dev/disk/by-id/nvme-Amazon_EC2_NVMe_Instance_Storage_AWS1F45C01D7E84FE3E9   # &lt;-- modify this line
        - /dev/disk/by-id/nvme-Amazon_EC2_NVMe_Instance_Storage_AWS6F45C01D7E84FE3E9   # &lt;-- modify this line
        - /dev/disk/by-id/nvme-Amazon_EC2_NVMe_Instance_Storage_AWS136BC945B4ECB9AE4   # &lt;-- modify this line
        - /dev/disk/by-id/nvme-Amazon_EC2_NVMe_Instance_Storage_AWS636BC945B4ECB9AE4   # &lt;-- modify this line</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can see that in this CR new <code>by-id</code> devices have been added. Each device maps to <code>nvme1n1</code> on one of three worker node.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>nvme-Amazon_EC2_NVMe_Instance_Storage_AWS60382E5D7441494EC</p>
</li>
<li>
<p>nvme-Amazon_EC2_NVMe_Instance_Storage_AWS6F45C01D7E84FE3E9</p>
</li>
<li>
<p>nvme-Amazon_EC2_NVMe_Instance_Storage_AWS636BC945B4ECB9AE4</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Create this LocalVolume CR using the following command:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc apply -f local-storage-block-expand.yaml</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>localvolume.local.storage.openshift.io/local-block configured</pre>
</div>
</div>
<div class="paragraph">
<p>Now that the CR is created let&#8217;s see the results.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get pv | grep localblock</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>local-pv-1d63db9e   1769Gi     RWO            Delete           Available           localblock              33s
local-pv-1eb9da0a   1769Gi     RWO            Delete           Available           localblock              25s
local-pv-31021a83   1769Gi     RWO            Delete           Available           localblock              48s
...</pre>
</div>
</div>
<div class="paragraph">
<p>Now there are 3 more <code>Available</code> <strong>PVs</strong> to add to our <strong>StorageCluster</strong>. To do the expansion the only modification to the StorageCluster CR is to modify the <code>count</code> for <strong>storageDeviceSets</strong> from <code>1</code> to <code>2</code>.</p>
</div>
<div class="listingblock">
<div class="title">StorageCluster CR cluster-service-AWS-expand.yaml using <code>gp2</code> and <code>localblock</code> storageclasses:</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: ocs.openshift.io/v1
kind: StorageCluster
metadata:
  name: ocs-storagecluster
  namespace: openshift-storage
spec:
  manageNodes: false
  monPVCTemplate:
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 10Gi
      storageClassName: gp2
      volumeMode: Filesystem
  storageDeviceSets:
  - count: 2   # &lt;-- modify count to 2
    dataPVCTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 1
        storageClassName: localblock
        volumeMode: Block
    name: ocs-deviceset
    placement: {}
    portable: false
    replica: 3
    resources: {}</code></pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The <code>storage</code> size for <strong>storageDeviceSets</strong> must be less than or equal to the size of the raw block devices. Setting the value to <code>1</code> will guarantee that this requirement is met.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Create this StorageCluster CR using the following command:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc apply -f https://raw.githubusercontent.com/red-hat-storage/ocs-training/master/ocp4ocs4/yamls/cluster-service-AWS-expand.yaml</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>storagecluster.ocs.openshift.io/ocs-storagecluster configured</pre>
</div>
</div>
<div class="paragraph">
<p>You should now have 3 more OSD <strong>Pods</strong> (osd-3, osd-4 and osd-5) and 3 more osd-prepare <strong>Pods</strong>.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get pods -n openshift-storage | grep 'ceph-osd'</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>...
rook-ceph-osd-3-568d8797b6-j5xqx                                  1/1     Running     0          14m
rook-ceph-osd-4-cc4747fdf-5glgl                                   1/1     Running     0          14m
rook-ceph-osd-5-94c46bbcc-tb7pw                                   1/1     Running     0          14m
...
rook-ceph-osd-prepare-ocs-deviceset-0-1-mcmlv-qmn4r               0/1     Completed   0          14m
rook-ceph-osd-prepare-ocs-deviceset-1-1-tjh2d-fl5zc               0/1     Completed   0          14m
rook-ceph-osd-prepare-ocs-deviceset-2-1-nqlkg-x9wdn               0/1     Completed   0          14m</pre>
</div>
</div>
<div class="paragraph">
<p>Reference <a href="#Validating the successful OCS Storage Cluster creation on AWS">[Validating the successful OCS Storage Cluster creation on AWS]</a> for how to validate your <strong>StorageCluster</strong> deployment.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_replacing_a_failed_device_with_openshift_container_storage"><a class="anchor" href="#_replacing_a_failed_device_with_openshift_container_storage"></a>5. Replacing a Failed Device with OpenShift Container Storage</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This process should be followed when an OSD <strong>Pod</strong> is in an <code>Error</code> state and the root cause is a failed underlying storage device.</p>
</div>
<div class="paragraph">
<p>Login to <strong>OpenShift Web Console</strong> and view the storage Dashboard.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-OCP-Dashboard-Status-Bad.png" alt="OCP Storage Dashboard status">
</div>
<div class="title">Figure 5. OCP Storage Dashboard status after OSD failed</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Make sure to have the Rook-Ceph <code>toolbox</code> <strong>Pod</strong> available. Instructions for deploying the <code>toolbox</code> can be found in <a href="#_using_the_rook_ceph_toolbox_to_validate_ceph_backing_storage">Using the Rook-Ceph toolbox to Validate Ceph backing storage</a>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="_removing_failed_osd_from_ceph_cluster"><a class="anchor" href="#_removing_failed_osd_from_ceph_cluster"></a>5.1. Removing failed OSD from Ceph cluster</h3>
<div class="paragraph">
<p>The first step is to identify the OCP node that has the bad OSD scheduled on it. In this example it is OCP node <code>compute-2</code>.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get -n openshift-storage pods -o wide | grep osd | grep -v prepare</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>rook-ceph-osd-0-6d77d6c7c6-m8xj6                                  0/1     CrashLoopBackOff        0          24h   10.129.0.16   compute-2   &lt;none&gt;           &lt;none&gt;
rook-ceph-osd-1-85d99fb95f-2svc7                                  1/1     Running     	        0          24h   10.128.2.24   compute-0   &lt;none&gt;           &lt;none&gt;
rook-ceph-osd-2-6c66cdb977-jp542                                  1/1     Running     	        0          24h   10.130.0.18   compute-1   &lt;none&gt;           &lt;none&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>Now that the OCP node has been identified you will log into the <code>toolbox</code> <strong>Pod</strong>.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">TOOLS_POD=$(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name)
oc rsh -n openshift-storage $TOOLS_POD</code></pre>
</div>
</div>
<div class="paragraph">
<p>Run the following command to get {osd-id} for the failed drive. The STATUS you are looking for is <strong>down</strong>. In this example it is <code>osd.0</code>.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph osd tree</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>ID  CLASS WEIGHT  TYPE NAME                            STATUS REWEIGHT PRI-AFF
 -1       0.29008 root default
 -4       0.09669     rack rack0
 -3       0.09669         host ocs-deviceset-0-0-nvs68
  0   hdd 0.09669             osd.0                      down  1.00000 1.00000
 -8       0.09669     rack rack1
 -7       0.09669         host ocs-deviceset-1-0-959rp
  1   hdd 0.09669             osd.1                        up  1.00000 1.00000
-12       0.09669     rack rack2
-11       0.09669         host ocs-deviceset-2-0-79j94
  2   hdd 0.09669             osd.2                        up  1.00000 1.00000</pre>
</div>
</div>
<div class="paragraph">
<p>The following process will remove the <strong>down</strong> OSD from the cluster so a new OSD can be added.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph osd out {osd-id}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>marked out osd.0.</pre>
</div>
</div>
<div class="paragraph">
<p>After the OSD is marked out the <code>OSD REWEIGHT RATIO</code> is set to <code>zero</code>. This will cause the data to migrate from this OSD to the remaining OSDs.</p>
</div>
<div class="admonitionblock caution">
<table>
<tr>
<td class="icon">
<i class="fa icon-caution" title="Caution"></i>
</td>
<td class="content">
<div class="paragraph">
<p>In the case of only three OSDs the data cannot migrate because there is only one OSD in each of the 3 availability zones and only 2 OSDs are operational.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph osd tree</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>ID  CLASS WEIGHT  TYPE NAME                            STATUS REWEIGHT PRI-AFF
 -1       0.29008 root default
 -4       0.09669     rack rack0
 -3       0.09669         host ocs-deviceset-0-0-nvs68
  0   hdd 0.09669             osd.0                      down        0 1.00000
 -8       0.09669     rack rack1
 -7       0.09669         host ocs-deviceset-1-0-959rp
  1   hdd 0.09669             osd.1                        up  1.00000 1.00000
-12       0.09669     rack rack2
-11       0.09669         host ocs-deviceset-2-0-79j94
  2   hdd 0.09669             osd.2                        up  1.00000 1.00000</pre>
</div>
</div>
<div class="paragraph">
<p>In the case where the data can be migrated off the OSD in a <code>Error</code> state, you will want to wait until all <strong>PGs</strong> are <code>active+clean</code>.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph pg stat</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output for all data (PGs) migrating off of OSD</div>
<div class="content">
<pre>192 pgs: 192 active+clean;
380 MiB data, 1015 MiB used, 1.5 TiB / 1.5 TiB avail;
1.2 KiB/s rd, 59 KiB/s wr, 8 op/s</pre>
</div>
</div>
<div class="paragraph">
<p>Now this OSD needs to be removed from the Ceph cluster.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph osd purge {osd-id} --yes-i-really-mean-it</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>purged osd.0</pre>
</div>
</div>
<div class="paragraph">
<p>Now check to see that the OSD is removed.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>ceph osd tree</pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output for 3 OSD cluster after osd.0 purged</div>
<div class="content">
<pre>ID  CLASS WEIGHT  TYPE NAME                            STATUS REWEIGHT PRI-AFF
 -1       0.19339 root default
 -4             0     rack rack0
 -3             0         host ocs-deviceset-0-0-nvs68
 -8       0.09669     rack rack1
 -7       0.09669         host ocs-deviceset-1-0-959rp
  1   hdd 0.09669             osd.1                        up  1.00000 1.00000
-12       0.09669     rack rack2
-11       0.09669         host ocs-deviceset-2-0-79j94
  2   hdd 0.09669             osd.2                        up  1.00000 1.00000</pre>
</div>
</div>
<div class="paragraph">
<p>You can now exit the toolbox by either pressing <span class="keyseq"><kbd>Ctrl</kbd>+<kbd>D</kbd></span> or by executing</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">exit</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_delete_pvc_resources_associated_with_failed_osd"><a class="anchor" href="#_delete_pvc_resources_associated_with_failed_osd"></a>5.2. Delete PVC resources associated with failed OSD</h3>
<div class="paragraph">
<p>First the <strong>DeviceSet</strong> must be identified that is associated with the failed OSD. In this example the <strong>PVC</strong> name is <code>ocs-deviceset-0-0-nvs68</code>.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get -o yaml -n openshift-storage deployment rook-ceph-osd-{osd-id} | grep ceph.rook.io/pvc</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>ceph.rook.io/pvc: ocs-deviceset-0-0-nvs68
ceph.rook.io/pvc: ocs-deviceset-0-0-nvs68</pre>
</div>
</div>
<div class="paragraph">
<p>Scale down failed OSD <strong>deployment</strong> to <code>replicas=0</code>. In this example the deployment name is <code>rook-ceph-osd-0</code>.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc scale -n openshift-storage deployment rook-ceph-osd-{osd-id} --replicas=0</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>deployment.extensions/rook-ceph-osd-0 scaled</pre>
</div>
</div>
<div class="paragraph">
<p>Now identify the <strong>PV</strong> associated with the <strong>PVC</strong> identified earlier. In this example the associated <strong>PV</strong> is <code>local-pv-d9c5cbd6</code>.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get -n openshift-storage pvc ocs-deviceset-0-0-nvs68</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>NAME                      STATUS        VOLUME              CAPACITY   ACCESS MODES   STORAGECLASS   AGE
ocs-deviceset-0-0-nvs68   Bound   local-pv-d9c5cbd6   100Gi      RWO            localblock     24h</pre>
</div>
</div>
<div class="paragraph">
<p>Now the failed device name needs to be identified. In this example the device name is <code>sdb</code>.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get pv local-pv-d9c5cbd6 -o yaml | grep path</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>path: /mnt/local-storage/localblock/sdb</pre>
</div>
</div>
<div class="paragraph">
<p>The next step is to identify the <code>prepare-pod</code> associated with the failed OSD.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc describe -n openshift-storage pvc ocs-deviceset-0-0-nvs68 | grep Mounted</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>Mounted By:    rook-ceph-osd-prepare-ocs-deviceset-0-0-nvs68-zblp7</pre>
</div>
</div>
<div class="paragraph">
<p>This <code>prepare-pod</code> must be deleted before the associated <strong>PVC</strong> can be removed.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc delete -n openshift-storage pod rook-ceph-osd-prepare-ocs-deviceset-0-0-nvs68-zblp7</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>pod "rook-ceph-osd-prepare-ocs-deviceset-0-0-nvs68-zblp7" deleted</pre>
</div>
</div>
<div class="paragraph">
<p>Now the <strong>PVC</strong> associated with the failed OSD can be deleted.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc delete -n openshift-storage pvc -n openshift-storage ocs-deviceset-0-0-nvs68</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>persistentvolumeclaim "ocs-deviceset-0-0-nvs68" deleted</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_replace_failed_drive_and_create_new_pv"><a class="anchor" href="#_replace_failed_drive_and_create_new_pv"></a>5.3. Replace failed drive and create new PV</h3>
<div class="paragraph">
<p>After the <strong>PVC</strong> associated with the failed drive is deleted, it is time to replace the failed drive and use this new drive to create a new OCP <strong>PV</strong>.</p>
</div>
<div class="paragraph">
<p>First step is to login to the OCP node with the failed drive and record the <code>/dev/disk/by-id/{id}</code> that is to be replaced. In this example the OCP node is <code>compute-2</code>.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc debug node/compute-2</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>Starting pod/compute-2-debug ...
To use host binaries, run `chroot /host`
Pod IP: 10.70.56.66
If you don't see a command prompt, try pressing enter.
sh-4.2# chroot /host</pre>
</div>
</div>
<div class="paragraph">
<p>Using the device name identified earlier, <code>sdb</code>, record the <code>/dev/disk/by-id/{id}</code> for use in the next step.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>sh-4.4# ls -alh /mnt/local-storage/localblock</pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>total 0
drwxr-xr-x. 2 root root 17 Apr  8 23:03 .
drwxr-xr-x. 3 root root 24 Apr  8 23:03 ..
lrwxrwxrwx. 1 root root 54 Apr  8 23:03 sdb -&gt; /dev/disk/by-id/scsi-36000c2962b2f613ba1f8f4c5cf952237</pre>
</div>
</div>
<div class="paragraph">
<p>Identify the device name for the new drive. In this example <code>sdd</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>sh-4.4# lsblk</pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>NAME                         MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda                            8:0    0   60G  0 disk
|-sda1                         8:1    0  384M  0 part /boot
|-sda2                         8:2    0  127M  0 part /boot/efi
|-sda3                         8:3    0    1M  0 part
`-sda4                         8:4    0 59.5G  0 part
  `-coreos-luks-root-nocrypt 253:0    0 59.5G  0 dm   /sysroot
sdb                            8:16   0  100G  0 disk
`-ceph--c1d5448f--d79b--4778--977c--49a6b50d700a-osd--block--f85be71c--98f5--49c3--bf6f--1f1e3645d251
                             253:1    0   99G  0 lvm
sdc                            8:32   0   10G  0 disk /var/lib/kubelet/pods/df23429b-6dad-4d8c-b705-22871ba979de/vol
sdd                            8:48   0  100G  0 disk</pre>
</div>
</div>
<div class="paragraph">
<p>Now identify the <code>/dev/disk/by-id/{id}</code> for the new drive and record for use in the next step.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>sh-4.2# ls -alh /dev/disk/by-id | grep sdd</pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>lrwxrwxrwx. 1 root root   9 Apr  9 20:45 scsi-36000c29f5c9638dec9f19b220fbe36b1 -&gt; ../../sdd
lrwxrwxrwx. 1 root root   9 Apr  9 20:45 wwn-0x6000c29f5c9638dec9f19b220fbe36b1 -&gt; ../../sdd</pre>
</div>
</div>
<div class="paragraph">
<p>After the new <code>/dev/disk/by-id/{id}</code> is available a new disk entry can be added to the <strong>LocalVolume</strong> CR.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get -n local-storage localvolume</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>NAME          AGE
local-block   25h</pre>
</div>
</div>
<div class="paragraph">
<p>Edit <strong>LocalVolume</strong> CR and remove or comment out failed device <code>/dev/disk/by-id/{id}</code> and add the new <code>/dev/disk/by-id/{id}</code>. In this example the new device is <code>/dev/disk/by-id/scsi-36000c29f5c9638dec9f19b220fbe36b1</code>.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc edit -n local-storage localvolume local-block</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>[...]
  storageClassDevices:
  - devicePaths:
    - /dev/disk/by-id/scsi-36000c29346bca85f723c4c1f268b5630
    - /dev/disk/by-id/scsi-36000c29134dfcfaf2dfeeb9f98622786
#   - /dev/disk/by-id/scsi-36000c2962b2f613ba1f8f4c5cf952237
    - /dev/disk/by-id/scsi-36000c29f5c9638dec9f19b220fbe36b1
    storageClassName: localblock
    volumeMode: Block
[...]</pre>
</div>
</div>
<div class="paragraph">
<p>Make sure to save the changes after editing using <kbd>:wq!</kbd>.</p>
</div>
<div class="paragraph">
<p>Validate that there is new <code>Available</code> <strong>PV</strong> of correct size and that the old <strong>PV</strong> is now in a <code>Released</code> state.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get pv | grep 100Gi</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>local-pv-3e8964d3                          100Gi      RWO            Delete           Bound       openshift-storage/ocs-deviceset-2-0-79j94   localblock                             25h
local-pv-414755e0                          100Gi      RWO            Delete           Bound       openshift-storage/ocs-deviceset-1-0-959rp   localblock                             25h
local-pv-b481410                           100Gi      RWO            Delete           Available                                               localblock                             3m24s
local-pv-d9c5cbd6                          100Gi      RWO            Delete           Released    openshift-storage/ocs-deviceset-0-0-nvs68   localblock</pre>
</div>
</div>
<div class="paragraph">
<p>Login to OCP node with failed device and remove the old symlink. Validate it is removed before proceeding.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc debug node/compute-2</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>Starting pod/compute-2-debug ...
To use host binaries, run `chroot /host`
Pod IP: 10.70.56.66
If you don't see a command prompt, try pressing enter.
sh-4.2# chroot /host</pre>
</div>
</div>
<div class="paragraph">
<p>Identify the old <code>symlink</code> for the failed device name. In this example the failed device name is <code>sdb</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>sh-4.4# ls -alh /mnt/local-storage/localblock</pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>total 0
drwxr-xr-x. 2 root root 28 Apr 10 00:42 .
drwxr-xr-x. 3 root root 24 Apr  8 23:03 ..
lrwxrwxrwx. 1 root root 54 Apr  8 23:03 sdb -&gt; /dev/disk/by-id/scsi-36000c2962b2f613ba1f8f4c5cf952237
lrwxrwxrwx. 1 root root 54 Apr 10 00:42 sdd -&gt; /dev/disk/by-id/scsi-36000c29f5c9638dec9f19b220fbe36b1</pre>
</div>
</div>
<div class="paragraph">
<p>Remove the <code>symlink</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>sh-4.4# rm /mnt/local-storage/localblock/sdb</pre>
</div>
</div>
<div class="paragraph">
<p>Validate the <code>symlink</code> is removed.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>sh-4.4# ls -alh /mnt/local-storage/localblock</pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>total 0
drwxr-xr-x. 2 root root 17 Apr 10 00:56 .
drwxr-xr-x. 3 root root 24 Apr  8 23:03 ..
lrwxrwxrwx. 1 root root 54 Apr 10 00:42 sdd -&gt; /dev/disk/by-id/scsi-36000c29f5c9638dec9f19b220fbe36b1</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_create_new_osd_for_new_device"><a class="anchor" href="#_create_new_osd_for_new_device"></a>5.4. Create new OSD for new device</h3>
<div class="paragraph">
<p>Start by deleting the <strong>PV</strong> associated with the failed device. This <strong>PV</strong> name was identified in an earlier step. In this example the <strong>PV</strong> name is <code>local-pv-d9c5cbd6</code>.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc delete pv local-pv-d9c5cbd6</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>persistentvolume "local-pv-d9c5cbd6" deleted</pre>
</div>
</div>
<div class="paragraph">
<p>Verify <strong>PV</strong> for the failed drive is now gone. There should still be an <code>Available</code> <strong>PV</strong> for the new drive.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get pv | grep 100Gi</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>local-pv-3e8964d3                          100Gi      RWO            Delete           Bound       openshift-storage/ocs-deviceset-2-0-79j94   localblock                             1d20h
local-pv-414755e0                          100Gi      RWO            Delete           Bound       openshift-storage/ocs-deviceset-1-0-959rp   localblock                             1d20h
local-pv-b481410                           100Gi      RWO            Delete           Available                                               localblock                             1d18h</pre>
</div>
</div>
<div class="paragraph">
<p>Next step is to delete the <strong>deployment</strong> for the failed OSD <strong>Pod</strong>. This <strong>deployment</strong> was scaled to <code>replicas=0</code> in an earlier step.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get -n openshift-storage deployments | grep osd</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>rook-ceph-osd-0                                      0/0     0            0           1d20h
rook-ceph-osd-1                                      1/1     1            1           1d20h
rook-ceph-osd-2                                      1/1     1            1           1d20h</pre>
</div>
</div>
<div class="paragraph">
<p>For this example the deployment name is <code>rook-ceph-osd-0</code>.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc delete -n openshift-storage deployment rook-ceph-osd-{osd-id}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>deployment.extensions "rook-ceph-osd-0" deleted</pre>
</div>
</div>
<div class="paragraph">
<p>Now that the <strong>deployment</strong> and all other associated OCP and Ceph resources for the failed device are deleted or removed, the new OSD can be deployed. This is done by restarting the <code>rook-ceph-operator</code> to force operator reconciliation.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get -n openshift-storage pod -l app=rook-ceph-operator</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>NAME                                  READY   STATUS    RESTARTS   AGE
rook-ceph-operator-6f74fb5bff-2d982   1/1     Running   0          1d20h</pre>
</div>
</div>
<div class="paragraph">
<p>Now delete the <code>rook-ceph-operator</code>.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc -n openshift-storage delete pod rook-ceph-operator-6f74fb5bff-2d982</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>pod "rook-ceph-operator-6f74fb5bff-2d982" deleted</pre>
</div>
</div>
<div class="paragraph">
<p>Now validate the <code>rook-ceph-operator</code> <strong>Pod</strong> is restarted.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get -n openshift-storage pod -l app=rook-ceph-operator</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>NAME                                  READY   STATUS    RESTARTS   AGE
rook-ceph-operator-6f74fb5bff-7mvrq   1/1     Running   0          66s</pre>
</div>
</div>
<div class="paragraph">
<p>Last step is to validate there is a new OSD, that Ceph is healthy, and that a successful replacement shows in the <strong>OpenShift Web Console</strong> Dashboards.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc -n openshift-storage get pods | grep osd | grep -v prepare</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>rook-ceph-osd-0-5f7f4747d4-snshw                                  1/1     Running     0          4m47s
rook-ceph-osd-1-85d99fb95f-2svc7                                  1/1     Running     0          1d20h
rook-ceph-osd-2-6c66cdb977-jp542                                  1/1     Running     0          1d20h</pre>
</div>
</div>
<div class="paragraph">
<p>There now is a OSD that was redeployed with a similar name, <code>rook-ceph-osd-0</code>.</p>
</div>
<div class="paragraph">
<p>Next step is to login to Ceph and see if the cluster is healthy.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">TOOLS_POD=$(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name)
oc rsh -n openshift-storage $TOOLS_POD</code></pre>
</div>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph status</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>  cluster:
    id:     fc89e00e-959e-486b-aff1-d9734778e9e0
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum a,b,c (age 2d)
    mgr: a(active, since 2d)
    mds: ocs-storagecluster-cephfilesystem:1 {0=ocs-storagecluster-cephfilesystem-a=up:active} 1 up:standby-replay
    osd: 3 osds: 3 up (since 11m), 3 in (since 11m)
    rgw: 1 daemon active (ocs.storagecluster.cephobjectstore.a)

  task status:

  data:
    pools:   10 pools, 192 pgs
    objects: 479 objects, 673 MiB
    usage:   4.9 GiB used, 292 GiB / 297 GiB avail
    pgs:     192 active+clean

  io:
    client:   853 B/s rd, 38 KiB/s wr, 1 op/s rd, 5 op/s wr</pre>
</div>
</div>
<div class="paragraph">
<p>We can see the Ceph health is <code>HEALTH_OK</code>.</p>
</div>
<div class="paragraph">
<p>You can now exit the toolbox by either pressing <span class="keyseq"><kbd>Ctrl</kbd>+<kbd>D</kbd></span> or by executing</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">exit</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now login to <strong>OpenShift Web Console</strong> and view the storage Dashboard.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-OCP-Dashboard-Status.png" alt="OCP Storage Dashboard status">
</div>
<div class="title">Figure 6. OCP Storage Dashboard status after OSD replacement</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_using_the_rook_ceph_toolbox_to_validate_ceph_backing_storage"><a class="anchor" href="#_using_the_rook_ceph_toolbox_to_validate_ceph_backing_storage"></a>6. Using the Rook-Ceph toolbox to Validate Ceph backing storage</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Starting with OpenShift Container Storage 4.3 the deployment of a <strong>toolbox</strong> can be created by modifying the <strong>CustomResource</strong> <code>OCSInitialization</code>.</p>
</div>
<div class="paragraph">
<p>You can either patch the <code>OCSInitialization ocsinit</code> using the following command line:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc patch OCSInitialization ocsinit -n openshift-storage --type json --patch  '[{ "op": "replace", "path": "/spec/enableCephTools", "value": true }]'</code></pre>
</div>
</div>
<div class="paragraph">
<p>Or you can edit the <code>OCSInitialization ocsinit</code> to toggle the <code>enableCephTools</code> parameter to <strong>true</strong> using the following command line:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc edit OCSInitialization ocsinit</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>spec</code> item must be set to the following value:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">spec:
  enableCephTools: true</code></pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Toggling the value from <code>true</code> to <code>false</code> will terminate any running <strong>toolbox</strong> pod immediately.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>After the <code>rook-ceph-tools</code> <strong>Pod</strong> is <code>Running</code> you can access the toolbox like this:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">TOOLS_POD=$(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name)
oc rsh -n openshift-storage $TOOLS_POD</code></pre>
</div>
</div>
<div class="paragraph">
<p>Once inside the toolbox, try out the following Ceph commands to see the status of Ceph, the total number of OSDs (example below shows six after expanding storage), and the total amount of storage available in the cluster.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph status</code></pre>
</div>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph osd status</code></pre>
</div>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph osd tree</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">sh-4.2# ceph status
  cluster:
    id:     fb084de5-e7c8-47f4-9c45-e57953fc44fd
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum a,b,c (age 23m)
    mgr: a(active, since 42m)
    mds: ocs-storagecluster-cephfilesystem:1 {0=ocs-storagecluster-cephfilesystem-b=up:active} 1 up:standby-replay
    osd: 6 osds: 6 up (since 22m), 6 in (since 22m)

  data:
    pools:   3 pools, 136 pgs
    objects: 95 objects, 94 MiB
    usage:   6.1 GiB used, 10 TiB / 10 TiB avail
    pgs:     136 active+clean

  io:
    client:   853 B/s rd, 25 KiB/s wr, 1 op/s rd, 3 op/s wr</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can exit the toolbox by either pressing <span class="keyseq"><kbd>Ctrl</kbd>+<kbd>D</kbd></span> or by executing</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">exit</code></pre>
</div>
</div>
</div>
</div>
</article>
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
  </div>
</main>
</div>
<footer class="footer">
  <p>This page was built using the Antora default UI.</p>
  <p>The source code for this UI is licensed under the terms of the MPL-2.0 license.</p>
</footer>
<script src="../../_/js/site.js"></script>
<script async src="../../_/js/vendor/highlight.js"></script>
  </body>
</html>
